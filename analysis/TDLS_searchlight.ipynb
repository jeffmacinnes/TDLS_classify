{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from nilearn.image import index_img\n",
    "from nilearn.image import load_img\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.image import new_img_like\n",
    "from nilearn.decoding import SearchLight\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification \n",
    "This notebook runs SVM-based searchlights across a variety of classification problems. For a detailed walkthrough of the individual steps for setting up a single classifier, see TDLS_sandbox.ipynb in this same directory\n",
    "\n",
    "## fMRI preprocessing\n",
    "For this analysis, the functional data has been preprocessed with the following steps:\n",
    "\n",
    "* motion correction\n",
    "* non-brain removal with BET (along with func mask creation)\n",
    "* 4mm smoothing\n",
    "* highpass filtered (Gaussian least-squares straight-line fitting, sigma=40.0s)\n",
    "\n",
    "In addition, we ran first level GLMs to extract SPMs that represented single trial parameter estimates. For each trial, a model was fit with two regressors: \n",
    "1. convolved regressor representing THAT trial\n",
    "2. convolved regressor representing ALL OTHER trials. \n",
    "\n",
    "The unique parameter estimate map for each trial was extracted and merged across time to produce a 4D file. There are 48 'timepts' in this 4D file, each representing the whole brain parameter estimate map for a single trial\n",
    "\n",
    "The 4D single-trial parameter maps will be used as the input dataset for our classification\n",
    "This experiment was a single run task with 48 trials. On each trial, subjects were presented with a stimulus in the form of either a **Word** or a **Picture** (24 stims from each modality).\n",
    "\n",
    "\n",
    "## Task info\n",
    "Stimuli represented either **Dwellings** or **Tools** (equally balanced across modalities). \n",
    "\n",
    "There were 8 unique stimuli. \n",
    "\n",
    "** Stimuli Breakdown:**\n",
    "\n",
    "* 24 Words\n",
    "    * 12 Dwellings\n",
    "        * 4 stims (as words), repeated 3x each\n",
    "    * 12 Tools \n",
    "        * 4 stims (as words), repeated 3x each\n",
    "* 24 Pics\n",
    "    * 12 Dwellings\n",
    "        * 4 stims (as pics), repeated 3x each\n",
    "    * 12 Tools\n",
    "        * 4 stims (as pics), repeated 3x each\n",
    "\n",
    "\n",
    "## Classifiers\n",
    "For each subject, we run the following searchlights across the whole brain, attempting to classify between\n",
    "* Modality (word vs picture overall)\n",
    "* Category (dwelling vs tool overall)\n",
    "* Stimulus (unique stimulus overall)\n",
    "* categoryWords (dwelling vs tool, words only)\n",
    "* categoryPics(dwelling vs tool, pics only)\n",
    "* stimulusWords (unique stim, words only)\n",
    "* stimulusPics (unique stim, pics only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the data directory for all subjects\n",
    "dataDir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Methods for prepping target labels \n",
    "def catByWord(row):\n",
    "    if row['Modality'] == 'Word':\n",
    "        label = row['Category'] + 'Word'\n",
    "    else:\n",
    "        label = 'n'\n",
    "    return label\n",
    "\n",
    "def catByPic(row):\n",
    "    if row['Modality'] == 'Picture':\n",
    "        label = row['Category'] + 'Pic'\n",
    "    else:\n",
    "        label = 'n'\n",
    "    return label\n",
    "\n",
    "def stimByWord(row):\n",
    "    if row['Modality'] == 'Word':\n",
    "        label = row['Stimulus'] + 'Word'\n",
    "    else:\n",
    "        label = 'n'\n",
    "    return label\n",
    "\n",
    "def stimByPic(row):\n",
    "    if row['Modality'] == 'Picture':\n",
    "        label = row['Stimulus'] + 'Pic'\n",
    "    else:\n",
    "        label = 'n'\n",
    "    return label\n",
    "\n",
    "\n",
    "def prepTargets(trialOnsets_fname):\n",
    "    \"\"\"\n",
    "    Take the trial onsets table convert it into a table of target labels. \n",
    "    The resulting table will be [nFeatures x nClassifiers], where each col\n",
    "    represents the target labels for a given classification problem (e.g. words vs pics)\n",
    "    \"\"\"\n",
    "    # read the trial onsets\n",
    "    trialOnsets = pd.read_table(trialOnsets_fname)\n",
    "    \n",
    "    # drop the TrialOnset time column\n",
    "    trialLabels = trialOnsets.drop('TrialOnset', axis=1)\n",
    "    \n",
    "    # 'trials' already has columns labeling rows by Modality, Category, and Stimulus.\n",
    "    # add additional columns for the following labels\n",
    "    trialLabels['categoryWords'] = trialLabels.apply(catByWord, axis=1)\n",
    "    trialLabels['categoryPics'] = trialLabels.apply(catByPic, axis=1)\n",
    "    trialLabels['stimulusWords'] = trialLabels.apply(stimByWord, axis=1)\n",
    "    trialLabels['stimulusPics'] = trialLabels.apply(stimByPic, axis=1)\n",
    "    \n",
    "    return trialLabels\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAllClassifiers(subj):\n",
    "    \"\"\"\n",
    "    For a given subject, prep all input data\n",
    "    and loop over all classifications\n",
    "    \"\"\"\n",
    "    subjDataDir = join(dataDir, subj)\n",
    "    outputDir = join(dataDir, subj, 'searchLights')\n",
    "    if not os.path.isdir(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "        \n",
    "    # create a variable that will store a dataframe of all results for this subject and get returned to the\n",
    "    # parent function\n",
    "    subj_df = None\n",
    "    \n",
    "    ### input paths\n",
    "    singleTrialPEs_file = join(subjDataDir, 'singleTrialGLM/singleTrialPEs.nii.gz')  # features matrix\n",
    "    mask_file = join(subjDataDir, 'masks/TDSL2_brain_mask.nii.gz')  # functional data mask\n",
    "    trialOnsets_file = join(subjDataDir, (subj + '_trialOnsets.txt'))      # trial onsets file\n",
    "    \n",
    "    ### prep target labels\n",
    "    targets = prepTargets(trialOnsets_file)\n",
    "    \n",
    "    ### load mask\n",
    "    mask_img = load_img(mask_file)\n",
    "    \n",
    "    # Each column in 'targets' represents a different classification problem to try for this subj\n",
    "    for classProb in targets.columns:\n",
    "    #for classProb in ['stimulusWords']:\n",
    "        print('classifying subj {} based on {}'.format(subj, classProb))\n",
    "        \n",
    "        # get the target labels for this problem\n",
    "        targetLabels = targets[classProb]\n",
    "        \n",
    "        # for some classifiers, not all trials apply. E.g. categoryWords can exclude\n",
    "        # all trials that were pictures. So set up a mask for the samples\n",
    "        samplesMask = targetLabels != 'n'\n",
    "        \n",
    "        # load the nifti image (masking for specified samples only)\n",
    "        fmri = index_img(singleTrialPEs_file, samplesMask)\n",
    "        \n",
    "        # apply the same samples mask to the target labels\n",
    "        targetLabels = targetLabels[samplesMask]\n",
    "\n",
    "        # create mean image\n",
    "        mean_fmri = mean_img(fmri)\n",
    "        \n",
    "        # set the number of splits for the K-fold cross validation. This number \n",
    "        # has to be less than or equal to the number of members of each class. Thus, \n",
    "        # for stimulusWords, and stimulusPics, it has to be set to 3 or lower\n",
    "        if classProb in ['stimulusWords', 'stimulusPics']:\n",
    "            nSplits=3\n",
    "        else:\n",
    "            nSplits=5\n",
    "        \n",
    "        # run the searchlight\n",
    "        #try:\n",
    "        results = runSearchLight(niftiData=fmri, labels=targetLabels, mask=mask_img, nSplits=nSplits)\n",
    "\n",
    "        ### process the results\n",
    "        # max accuracy across whole brain\n",
    "        maxAccuracy = results.scores_.max()\n",
    "\n",
    "        # location of voxel with max accuracy\n",
    "        voxIdx = np.argmax(results.scores_)\n",
    "        voxCoords = np.unravel_index(voxIdx, results.scores_.shape)\n",
    "\n",
    "        radius = results.get_params()['radius']\n",
    "\n",
    "        # store whole brain map\n",
    "        results_img = new_img_like(mean_fmri, results.scores_)\n",
    "        output_fname = '{}_r{:.1f}_{}.nii.gz'.format(subj, radius, classProb)\n",
    "        results_img.to_filename(join(outputDir, output_fname))\n",
    "        \n",
    "        # store the relevant results in a dict\n",
    "        thisResult = pd.DataFrame({'Subj': subj,\n",
    "                      'Classification': classProb,\n",
    "                      'maxAcc': maxAccuracy,\n",
    "                      'maxVox':','.join([str(x) for x in voxCoords]),\n",
    "                      'radius': radius}, index=[0])\n",
    "\n",
    "        if subj_df is None:\n",
    "            subj_df = thisResult\n",
    "        else:\n",
    "            subj_df = pd.concat([subj_df, thisResult], ignore_index=True)\n",
    "\n",
    "        # print results\n",
    "        print('Subj {}:, {} - max acc {:.1f}%, {:.1f}mm radius sphere at {}'.format(subj, \n",
    "                                                                                    classProb, \n",
    "                                                                                    maxAccuracy*100, \n",
    "                                                                                    radius,\n",
    "                                                                                    voxCoords))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print('Error while classifying subj {} based on {}'.format(subj, classProb))\n",
    "#             print(e)\n",
    "            \n",
    "    # After running all classifiers for this subject, return the dataframe with all results\n",
    "    return subj_df\n",
    "\n",
    "                \n",
    "def runSearchLight(niftiData=None, labels=None, mask=None, nSplits=5):\n",
    "    \"\"\"\n",
    "    Run a searchlight classifier on the supplied dataset. \n",
    "    Inputs:\n",
    "        niftiData: a 4D nifit image (nibabel)\n",
    "        labels: labels you want to base classification on (len(labels) should equal the number of timepts in niftiData)\n",
    "        mask: mask nifti image (nibabel)\n",
    "        nSplits: number of splits in KFold (must be less than the )\n",
    "    \"\"\"   \n",
    "    # specify how you want to handle cross-validation. Here, a\n",
    "    # stratifiedKFold approach is used, which will ensure an even balance\n",
    "    # of samples from each category in the train and test sets\n",
    "    skf = StratifiedKFold(n_splits=nSplits, random_state=1)\n",
    "\n",
    "    # prep the searchlight object\n",
    "    radius = 5.0\n",
    "    searchlight = SearchLight(\n",
    "                mask, \n",
    "                radius=radius,\n",
    "                n_jobs=-1,\n",
    "                verbose=0,\n",
    "                scoring='accuracy',\n",
    "                cv=skf)\n",
    "\n",
    "    # run the search light (NOTE: this will make your CPU sing)\n",
    "    searchlight.fit(niftiData, labels)\n",
    "    \n",
    "    return searchlight\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifying subj 13034 based on Modality\n",
      "Subj 13034:, Modality - max acc 77.0%, 5.0mm radius sphere at (35, 9, 4)\n",
      "classifying subj 13034 based on Category\n",
      "Subj 13034:, Category - max acc 89.0%, 5.0mm radius sphere at (39, 13, 5)\n",
      "classifying subj 13034 based on Stimulus\n",
      "Subj 13034:, Stimulus - max acc 36.2%, 5.0mm radius sphere at (29, 51, 7)\n",
      "classifying subj 13034 based on categoryWords\n",
      "Subj 13034:, categoryWords - max acc 96.7%, 5.0mm radius sphere at (25, 28, 11)\n",
      "classifying subj 13034 based on categoryPics\n",
      "Subj 13034:, categoryPics - max acc 100.0%, 5.0mm radius sphere at (36, 39, 7)\n",
      "classifying subj 13034 based on stimulusWords\n",
      "Subj 13034:, stimulusWords - max acc 50.0%, 5.0mm radius sphere at (18, 28, 3)\n",
      "classifying subj 13034 based on stimulusPics\n",
      "Subj 13034:, stimulusPics - max acc 45.8%, 5.0mm radius sphere at (25, 26, 1)\n",
      "classifying subj 13035 based on Modality\n",
      "Subj 13035:, Modality - max acc 81.5%, 5.0mm radius sphere at (43, 18, 1)\n",
      "classifying subj 13035 based on Category\n",
      "Subj 13035:, Category - max acc 79.0%, 5.0mm radius sphere at (26, 30, 3)\n",
      "classifying subj 13035 based on Stimulus\n",
      "Subj 13035:, Stimulus - max acc 38.8%, 5.0mm radius sphere at (47, 33, 15)\n",
      "classifying subj 13035 based on categoryWords\n",
      "Subj 13035:, categoryWords - max acc 96.7%, 5.0mm radius sphere at (31, 46, 8)\n",
      "classifying subj 13035 based on categoryPics\n",
      "Subj 13035:, categoryPics - max acc 100.0%, 5.0mm radius sphere at (39, 40, 9)\n",
      "classifying subj 13035 based on stimulusWords\n",
      "Subj 13035:, stimulusWords - max acc 54.2%, 5.0mm radius sphere at (46, 21, 10)\n",
      "classifying subj 13035 based on stimulusPics\n",
      "Subj 13035:, stimulusPics - max acc 45.8%, 5.0mm radius sphere at (23, 53, 5)\n",
      "classifying subj 13036 based on Modality\n",
      "Subj 13036:, Modality - max acc 85.5%, 5.0mm radius sphere at (38, 10, 5)\n",
      "classifying subj 13036 based on Category\n",
      "Subj 13036:, Category - max acc 85.5%, 5.0mm radius sphere at (48, 30, 12)\n",
      "classifying subj 13036 based on Stimulus\n"
     ]
    }
   ],
   "source": [
    "subjs = ['13034', '13035', '13036',\n",
    "        '13038', '13039', '13040']\n",
    "\n",
    "allResults = None\n",
    "for subj in subjs:\n",
    "    subj_df =  runAllClassifiers(subj)\n",
    "    \n",
    "    if subj_df is not None:\n",
    "        # combine this subjects results with the rest\n",
    "        if allResults is None:\n",
    "            allResults = subj_df\n",
    "        else:\n",
    "            allResults = pd.concat([allResults, subj_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
